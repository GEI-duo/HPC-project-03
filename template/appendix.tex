\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Code changes}

\begin{code}{title=Main function CLI arguments, label=code:main}{CUDA}
int main(int argc, char *argv[])
{
    if (argc != 6)
    {
        printf("Usage: %s <block_x> <block_y> <size> <steps> <name_output_file>\n", argv[0]);
        return 1;
    }
    
    double r = ALPHA * DT / (DX * DY);
    unsigned int arg = 1;
    int nx, ny, steps, block_x, block_y;
    char *filename;
    float ms = 0;
    
    // EXECUTION CLI PARAMS
    block_x     = atoi(argv[arg++]),
    block_y     = atoi(argv[arg++]),
    nx = ny     = atoi(argv[arg++]),
    steps       = atoi(argv[arg++]),
    filename    = argv[arg++];
    
    ...
}
\end{code}

\begin{code}{title=CUDA grid and data initialization, label=code:cuda-init}{CUDA}
#define CHECK_CUDA_ERRORS                                       \
    cudaError_t err = cudaGetLastError();                       \
    if (err != cudaSuccess) {                                   \
        printf("CUDA Error: %s\n", cudaGetErrorString(err));    \
    }                                                           \

int main(int argc, char *argv[])
{
    ...

    double *d_grid, *d_new_grid;
    cudaMalloc(&d_grid, nx * ny * sizeof(double));
    cudaMalloc(&d_new_grid, nx * ny * sizeof(double));
    
    dim3 blockDim(block_x, block_y);
    dim3 gridDim(
        (ny + block_x - 1) / block_x, 
        (nx + block_y - 1) / block_y
    );
    
    CHECK_CUDA_ERRORS;
    
    ...
}
\end{code}

\begin{code}{title=CUDA event time calculation, label=code:cuda-event}{CUDA}
int main(int argc, char *argv[])
{
    ... Initialization 

    // TIMING :: START
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    cudaEventRecord(start);

    initialize_grid(...);
    solve_heat_equation(...);

    // TIMING :: END
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms, start, stop);

    ... Writing the grid

    printf("Execution Time = %.3fms for %dx%d grid and %d steps\n", ms, nx, ny, steps);

    // FREE
    cudaFree(d_grid);
    cudaFree(d_new_grid);
    cudaEventDestroy(start);
    cudaEventDestroy(stop);

    return 0;
}
\end{code}

\begin{code}{title=Initialize grid, label=code:grid-init}{CUDA}
__global__ 
void initialize_grid_kernel(
    double *grid, 
    int nx, 
    int ny
)
{
    // Initialize the grid diagonals to be heat sources.
    // 
    // Steps:
    //  1. Identify executing code with i/j.
    //  2. Initialize the diagonal heat grid, excluding the matrix borders.
    
    int i = blockIdx.y * blockDim.y + threadIdx.y + 1,
        j = blockIdx.x * blockDim.x + threadIdx.x;

    // Ensure safe CUDA memory access
    if (i > nx || j >= ny) return;

    int is_edge     = i == 0 || i == nx - 1 || j == 0 || j == ny - 1, 
        is_diagonal = i == j || i == nx - j - 1;

    grid[i * ny + j] = !is_edge && is_diagonal ? T : 0.0;
}

void initialize_grid(
    double *grid,
    int nx,
    int ny,
    const dim3 gridDim,
    const dim3 blockDim
)
{
    initialize_grid_kernel<<<gridDim, blockDim>>>(grid, nx, ny);
    cudaDeviceSynchronize();
}
\end{code}

\begin{code}{title=Heat diffusion orchestrator, label=code:heat-calculation-orchestrator}{CUDA}
void solve_heat_equation(
    double *d_grid, 
    double *d_new_grid, 
    int nx,
    int ny,
    int steps,
    double r,
    const dim3 gridDim,
    const dim3 blockDim
)
{
    for (int step = 0; step < steps; step++)
    {
        heat_kernel<<<gridDim, blockDim>>>(d_grid, d_new_grid, nx, ny, r);
        cudaDeviceSynchronize();
        apply_boundary_conditions<<<gridDim, blockDim>>>(d_grid, nx, ny);
        cudaDeviceSynchronize();

        double *temp = d_grid;
        d_grid = d_new_grid;
        d_new_grid = temp;
    } 
}
\end{code}

\begin{code}{title=Heat diffusion calculation and boundary conditions, label=code:heat-calculation}{CUDA}
__global__ 
void heat_kernel(
    double* grid, 
    double* new_grid, 
    int nx, 
    int ny, 
    double r
) {
    int i = blockIdx.y * blockDim.y + threadIdx.y;
    int j = blockIdx.x * blockDim.x + threadIdx.x;

    if (i <= 0 || i >= nx - 1 || j <= 0 || j >= ny - 1) return;

    int idx = i * ny + j;

    new_grid[idx] = grid[idx]
        + r * (grid[idx + ny] + grid[idx - ny] - 2.0 * grid[idx])  // vertical
        + r * (grid[idx + 1] + grid[idx - 1] - 2.0 * grid[idx]);   // horizontal
}

__global__ 
void apply_boundary_conditions(
    double* grid, 
    int nx, 
    int ny
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < ny) {
        grid[0 * ny + idx] = 0.0;
        grid[(nx - 1) * ny + idx] = 0.0;
    }

    if (idx < nx) {
        grid[idx * ny + 0] = 0.0;
        grid[idx * ny + (ny - 1)] = 0.0;
    }
}
\end{code}

\begin{code}{title=Parallel pixel color calculation orchestrator, label=code:write-grid-orchestrator}{CUDA}
void write_grid(
    FILE* file, 
    double* d_grid, 
    int nx, 
    int ny
) {
    int row_stride = ny * 3;
    int padding = (4 - (row_stride % 4)) % 4;
    int padded_row_size = row_stride + padding;
    int total_size = nx * padded_row_size;

    // Allocate output buffer on GPU
    unsigned char* d_pixel_data;
    cudaMalloc(&d_pixel_data, total_size);

    dim3 writeBlockDim(16, 16);
    dim3 writeGridDim(
        (ny + writeBlockDim.x - 1) / writeBlockDim.x,
        (nx + writeBlockDim.y - 1) / writeBlockDim.y
    );

    CHECK_CUDA_ERRORS;

    prepare_pixel_data<<<writeGridDim, writeBlockDim>>>(d_grid, d_pixel_data, nx, ny, padded_row_size, padding);
    cudaDeviceSynchronize();

    // Copy back to host
    unsigned char* h_pixel_data = (unsigned char*)malloc(total_size);
    cudaMemcpy(h_pixel_data, d_pixel_data, total_size, cudaMemcpyDeviceToHost);

    fwrite(h_pixel_data, 1, total_size, file);

    cudaFree(d_pixel_data);
    free(h_pixel_data);
}
\end{code}

\begin{code}{title=Parallel pixel color calculation, label=code:prepare-pixel}{CUDA}
__device__ // GPU only function !! 
void get_color(
    double value, 
    unsigned char *r, 
    unsigned char *g, 
    unsigned char *b
)
{
...
}

__global__ 
void prepare_pixel_data(
    double* grid,
    unsigned char* pixel_data,
    int nx,
    int ny,
    int padded_row_size,
    int padding
) 
{
    int i = blockIdx.y * blockDim.y + threadIdx.y,
        j = blockIdx.x * blockDim.x + threadIdx.x;

    if (i >= nx || j >= ny) return;

    int row_index           = nx - 1 - i,
        row_offset          = i * padded_row_size,
        pixel_offset        = row_offset + j * 3,
        pixel_row_offset    = row_index * padded_row_size;


    unsigned char r, g, b;
    get_color(grid[row_index * ny + j], &r, &g, &b);

    pixel_data[pixel_offset + 0] = b;
    pixel_data[pixel_offset + 1] = g;
    pixel_data[pixel_offset + 2] = r;

    // Only one thread per row handles padding
    if (j == 0 && padding > 0) {
        for (int p = 0; p < padding; ++p) {
            pixel_data[pixel_row_offset + ny * 3 + p] = 0;
        }
    }
}
\end{code}

\section{Makefile and Python runner}

\begin{code}{title=Makefile, label=code:makefile}{Bash}
compile:
	nvcc -o heat_cuda heat.cu
	
compile_prod:
	nvcc -U CHECK_CUDA_ERRORS heat.cu -o heat_cuda

sample: compile
	./heat_cuda 2000 10000 sample.bmp

all: clear compile_prod
	python run.py -r ./results.csv

clear:
	rm -fr ./results/* results.csv
\end{code}

\begin{code}{title=Python runner, label=code:runner}{Python}
# ...imports and helpers

def main() -> None:
    block_dim = [(4, 256), (8, 126), (16, 64), (32, 32), (64, 16), (128, 8), (256, 4)]
    steps_dim = [100, 1_000, 10_000, 100_000]
    size_dim = [100, 1_000, 2_000]
    combinations = list(product(block_dim, steps_dim, size_dim))

    with open(args.results, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Block Dim", "Size", "Steps", "Time"])
        for (block_x, block_y), steps, size in combinations:
            output_filename = f"./results/out_b{block_x}x{block_y}_s{size}_t{steps}.bmp"

            command = [
                "./heat_cuda",
                str(block_x),
                str(block_y),
                str(size),
                str(steps),
                output_filename,
            ]

            _print(f"Running: {' '.join(command)}")
            try:
                result = subprocess.run(
                    command, capture_output=True, text=True, check=True
                )

                _print(result.stdout)
                time = extract_time(result.stdout)
                _print(f"{time}\n")

                writer.writerow([block_x, block_y, size, steps, time])
            except subprocess.CalledProcessError as e:
                print(f"Error during execution: {e}")
                exit(1)
# ... main call
\end{code}

\section{Plain results}

\begin{longtable}{|c|c|c|c|c|}
\hline
Block X & Block Y & Size & Steps  & Time       \\ \hline
\endfirsthead
%
\endhead
%
\hline
\endfoot
%
\endlastfoot
%
4       & 256     & 100  & 100    & 458.301    \\
4       & 256     & 1000 & 100    & 474.464    \\
4       & 256     & 2000 & 100    & 505.127    \\
4       & 256     & 100  & 1000   & 4546.784   \\
4       & 256     & 1000 & 1000   & 4706.944   \\
4       & 256     & 2000 & 1000   & 5014.381   \\
4       & 256     & 100  & 10000  & 45698.949  \\
4       & 256     & 1000 & 10000  & 47047.078  \\
4       & 256     & 2000 & 10000  & 49992.352  \\
4       & 256     & 100  & 100000 & 445655.594 \\
4       & 256     & 1000 & 100000 & 468449.812 \\
4       & 256     & 2000 & 100000 & 501137.031 \\
8       & 126     & 100  & 100    & 421.156    \\
8       & 126     & 1000 & 100    & 475.849    \\
8       & 126     & 2000 & 100    & 511.239    \\
8       & 126     & 100  & 1000   & 4582.521   \\
8       & 126     & 1000 & 1000   & 4710.59    \\
8       & 126     & 2000 & 1000   & 5033.29    \\
8       & 126     & 100  & 10000  & 40571.789  \\
8       & 126     & 1000 & 10000  & 47047.141  \\
8       & 126     & 2000 & 10000  & 50255.668  \\
8       & 126     & 100  & 100000 & 441985.531 \\
8       & 126     & 1000 & 100000 & 469245.438 \\
8       & 126     & 2000 & 100000 & 502696.188 \\
16      & 64      & 100  & 100    & 462.765    \\
16      & 64      & 1000 & 100    & 477.476    \\
16      & 64      & 2000 & 100    & 514.465    \\
16      & 64      & 100  & 1000   & 4516.707   \\
16      & 64      & 1000 & 1000   & 4722.947   \\
16      & 64      & 2000 & 1000   & 5081.395   \\
16      & 64      & 100  & 10000  & 45720.352  \\
16      & 64      & 1000 & 10000  & 47234.938  \\
16      & 64      & 2000 & 10000  & 50670.863  \\
16      & 64      & 100  & 100000 & 447631.062 \\
16      & 64      & 1000 & 100000 & 469904.469 \\
16      & 64      & 2000 & 100000 & 508339.656 \\
32      & 32      & 100  & 100    & 461.52     \\
32      & 32      & 1000 & 100    & 483.703    \\
32      & 32      & 2000 & 100    & 549.235    \\
32      & 32      & 100  & 1000   & 4583.285   \\
32      & 32      & 1000 & 1000   & 4828.854   \\
32      & 32      & 2000 & 1000   & 5450.304   \\
32      & 32      & 100  & 10000  & 45743.98   \\
32      & 32      & 1000 & 10000  & 48270.859  \\
32      & 32      & 2000 & 10000  & 54483.594  \\
32      & 32      & 100  & 100000 & 444581.562 \\
32      & 32      & 1000 & 100000 & 481960.656 \\
32      & 32      & 2000 & 100000 & 544399.0   \\
64      & 16      & 100  & 100    & 461.066    \\
64      & 16      & 1000 & 100    & 484.503    \\
64      & 16      & 2000 & 100    & 546.084    \\
64      & 16      & 100  & 1000   & 4581.335   \\
64      & 16      & 1000 & 1000   & 4831.66    \\
64      & 16      & 2000 & 1000   & 5443.014   \\
64      & 16      & 100  & 10000  & 41647.891  \\
64      & 16      & 1000 & 10000  & 48267.117  \\
64      & 16      & 2000 & 10000  & 54364.484  \\
64      & 16      & 100  & 100000 & 445172.906 \\
64      & 16      & 1000 & 100000 & 481990.406 \\
64      & 16      & 2000 & 100000 & 543602.938 \\
128     & 8       & 100  & 100    & 425.976    \\
128     & 8       & 1000 & 100    & 488.536    \\
128     & 8       & 2000 & 100    & 547.101    \\
128     & 8       & 100  & 1000   & 4578.743   \\
128     & 8       & 1000 & 1000   & 4832.119   \\
128     & 8       & 2000 & 1000   & 5444.092   \\
128     & 8       & 100  & 10000  & 45650.367  \\
128     & 8       & 1000 & 10000  & 47987.488  \\
128     & 8       & 2000 & 10000  & 54456.543  \\
128     & 8       & 100  & 100000 & 448711.312 \\
128     & 8       & 1000 & 100000 & 481643.312 \\
128     & 8       & 2000 & 100000 & 544161.312 \\
256     & 4       & 100  & 100    & 461.954    \\
256     & 4       & 1000 & 100    & 487.337    \\
256     & 4       & 2000 & 100    & 551.132    \\
256     & 4       & 100  & 1000   & 4541.861   \\
256     & 4       & 1000 & 1000   & 4829.228   \\
256     & 4       & 2000 & 1000   & 5444.631   \\
256     & 4       & 100  & 10000  & 45727.109  \\
256     & 4       & 1000 & 10000  & 48286.688  \\
256     & 4       & 2000 & 10000  & 54371.23   \\
256     & 4       & 100  & 100000 & 448256.438 \\
256     & 4       & 1000 & 100000 & 481991.188 \\
256     & 4       & 2000 & 100000 & 543877.25  \\ \hline
\end{longtable}

\end{document}